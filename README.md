# Tailscale Lab for hybrid connectivity to AWS EKS

## Summary
This first lab evaluates the following:
- Set up a CI/CD pipeline that deploys the hybrid AWS environment using Tailscale for connectivity.
- Deploy the AWS environment (EKS + VM) and bootstrap as much as possible (EKS cluster and basic Tailscale onboarding).
- Use basic tools to confirm connectivity and measure latency (ping/icmp).

Out of scope for this lab:
- Harden the EKS setup for production (avoid public access/endpoints and adopt a minimalist approach instead of using the default AWS module).
- Automate the initial onboarding of Tailscale; the lab uses a pre-existing *tsnet*.
- Use iperf3 to test bandwidth when Tailscale connects via a subnet router instead of public DERP.
- Integrate Tailscale logging with Grafana for performance analysis/visualization.

**Note:** This repository first commit was 100% generated by GitHub CoPilot in less than 15m with less than 5 prompts detailing design and its modules: secrets, runner and the lab itself. Another day of work was spent to review code, test, troubleshoot and fix bugs before the lab could be finally be leveraged to achieve the main goal to verify Tailscale newtorking in AWS hybrid environment with NAT gateway.


## LAB Topology
This is the original version generated by AI and rendered by **Graphviz DOT**. It is kept here for comparison purposes :)

![Lab Topology 1](./lab/topology/topology.png)


A slightly improved version (made by humans with draw.io):
![Lab Topology 2](./lab/topology/drawio_topology.png)


## Repository Structure

### Folders created:
- `terraform/runner`: Terraform to launch an EC2 instance to serve as a self-hosted GitHub Actions runner.
- `terraform/github-secrets`: Terraform to create GitHub Actions repository secrets to store AWS and Tailscale credentials for the pipeline. This workspace implies a local `terraform apply` to upload secrets.

### Notes and caveats:
- Set credentials via environment variables or `terraform.tfvars` when running locally. For the GitHub secrets module you must provide a `github_token` with sufficient scope and `github_owner` + `repository` where the secrets will be stored.
- The GitHub Actions workflow reads AWS credentials from repository secrets named `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_SESSION_TOKEN`. Tailscale credentials are passed as TF_vars.
- The `terraform/lab` module creates two VPCs in Oregon (`us-west-2`). The `server` VPC contains an EKS cluster (small nodes), a simple "hello world" deployment, and the Tailscale Operator. The `client` VPC contains a Linux VM with the Tailscale client installed and configured using `TF_VAR_tailscale_auth_key`.

- The `terraform/lab` Terraform scope details:
    - Builds two VPCs (`server` and `client`).
    - Creates an EKS cluster in the `server` VPC and deploys a basic `hello` app and the Tailscale Operator via Helm.
    - Creates a Linux VM in the `client` VPC that connects to the internet and runs the Tailscale client.
    - Provides an optional Tailscale subnet-router instance (toggle with `TF_VAR_tailscale_subnet_router_enable`). When enabled, the router will advertise the `client` VPC private subnets to Tailscale using `TF_VAR_tailscale_auth_key`.

- The EKS module depends on community modules (`terraform-aws-modules/eks/aws`) and the VPC module (`terraform-aws-modules/vpc/aws`). These modules simplify code but can be very complex as its behavior changes frequently across versions.
- Tailscale auth keys are sensitive: set `TF_VAR_tailscale_auth_key` in the environment or a secure `terraform.tfvars` file.
- This lab is intentionally permissive from an IAM perspective. Review IAM and security-group settings before using it in your environment.
- The runner EC2 will require admin permissions for AWS services: VPC, EC2, EKS, KMS, SSM, IAM, and CloudWatch Logs.


### Quick local example (runner)
```bash
cd terraform/runner
terraform init
terraform plan
terraform apply
```

### Quick local example (github-secrets)
```bash
cd terraform/github-secrets
export TF_VAR_github_token="ghp_..."
export TF_VAR_github_owner="my-org"
export TF_VAR_repository="my-repo"
export TF_VAR_aws_access_key_id="..."
export TF_VAR_aws_secret_access_key="..."
export TF_VAR_aws_session_token="..." # optional
terraform init
terraform apply
```

### Quick example to run with GitHub Actions
Reference: https://docs.github.com/en/actions/how-tos/manage-workflow-runs/manually-run-a-workflow
Make sure to upload the GitHub repository secrets before proceeding.

Enter the following variables:
```
Terraform target subfolder: lab
Terraform operation: plan/apply/destroy
AWS region: us-west-2
S3 bucket for Terraform remote state: bucket-name
```

## Lessons learned while using GitHub Copilot
1. GitHub Copilot generated a usable initial draft with a few prompts, but where context was missing it suggested approaches that were incomplete or incompatible (for example, non-standard ways to install Tailscale via Helm).
2. The assistant sometimes confused variable escaping and interpolation between Terraform `user_data` templates and Bash, which caused bugs.
3. The AI assistant references were somewhat outdated white setting up the AWS modules. Addition code review was necessary to address incompatibilities across selected versions.
4. The generated documentation and topology diagram were helpful as a starting point.
5. When troubleshooting EKS deployments, avoid prolonged ad-hoc kubectl changes that can create cascading issues; instead start from a clear, minimal configuration and iterate back with the AI assistance as needed.
6. (Updated): During the subnet-router design review, some GitHub Copilot suggestions produced hallucinations (non-existing Terraform resources or arguments). Be cautious and validate provider schemas when applying provider-specific resources.


## Resources
1. https://tailscale.com/kb/1210/terraform-provider
2. https://tailscale.com/blog/kubernetes-direct-connections
3. https://tailscale.com/kb/1236/kubernetes-operator
4. https://github.com/tailscale/tailscale/blob/main/cmd/k8s-operator/deploy/manifests/operator.yaml
5. https://github.com/rrotaru/eks-over-tailscale/tree/main
6. https://github.com/jaxxstorm/tailscale-examples
7. https://tailscale.com/kb/1406/quick-guide-subnets
8. https://tailscale.com/kb/1438/kubernetes-operator-cluster-egress#access-an-ip-address-behind-a-subnet-router
9. https://tailscale.com/kb/1214/site-to-site


## What already got a PASS
1. Terraform code for installing secrets, deploying runner and building AWS environment
2. Basic end-to-end connectivity between all peers via Tailscale
3. Move Tailscale ACL configs to IaC

## WIP
1. End-to-end testing using subnet router + ExternalName Service config
2. Move Tailscale relevant tag settings to IaC

## Possible improvements
1. Validate steps to identify NAT types (with/without NAT gateway + relay).
2. ~~Verify whether client–EKS server communications can use the VPC peering path instead of double-NAT + relay~~ — currently not supported as initially intended.
3. Evaluate the impact on latency when using subnet routers instead of relays.
4. Move Tailscale UI settings to IaC.
5. Automate connectivity tests.
6. Add logging and monitoring with Grafana.

