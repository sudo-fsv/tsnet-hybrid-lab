# Tailscale LAB for hybrid connectivity to AWS EKS

## Summary 
The goal of this (very first) lab is to evaluate the following:
* Set a CI/CD pipeline that will deploy the hybrid AWS environment using Tailscale for connectivity
* Deploy the AWS environment (EKS + VM) and bootstrap as much as possible (both the EKS cluster as well as the basic Tailscale onboarding).
* Include quick code toggles in the pipeline to avoid hard NAT.
* Use basic tools to confirm connectivity and evaluate latency: ping/icmp.
* Leverage iperf3 to test bandwidth when Tailscale connects over peering instead of IGW.

It is not in the scope of this Lab:
* Harden the EKS setup as much as possible: avoid public access/endpoints and adopt a more minimalist approach instead of using the default AWS module.
* Automate the initial onboarding of Tailscale. The lab uses a pre-existing *tsnet*.
* Integrate Taiscale logging with Grafana to improve the performance analysis/visualization.

**Note:** This repository first commit was 100% generated by GitHub CoPilot in less than 15m with less than 5 prompts detailing design and the separate modules: secrets, runner and the lab itself. It took me another day of work to review code, test, troubleshoot and fix bugs before I could finally leverage the lab to achieve the main goal to validate networking stats of Tailscale in AWS hybrid environment over hard and easy NATs.


## LAB Topology
This is the original version generated by AI and rendered by **Graphviz DOT**. It is kept here for comparison purposes :)

![Lab Topology 1](./lab/topology/topology.png)


A slightly improved version (made by humans with draw.io):
![Lab Topology 2](./lab/topology/drawio_topology.png)


## Repository Structure

### Folders created:
- `terraform/runner`: Terraform to launch an EC2 instance to serve as self-hosted GitActions runner. 
- `terraform/github-secrets`: Terraform to create GitHub Actions repository secrets to store AWS and Tailscale credentials that will be safely consumed by the pipeline. This workspace implies a local Terraform apply.

### Notes and caveats:
- Set credentials via environment variables or `terraform.tfvars` when running locally. For the GitHub secrets module you must provide a `github_token` with sufficient scope and `github_owner` + `repository` where the secrets will be stored.
- The GitHub Actions workflow reads AWS credentials from repository secrets named `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_SESSION_TOKEN` while the Tailscale credentials are passed as TF_vars.
- The lab module `terraform/lab` creates two VPCs in Oregon (`us-west-2`) and VPC peering between them. The `server` VPC contains an EKS cluster (small nodes) with a simple "hello world" deployment and the Tailscale Operator. The `client` VPC contains a Linux VM with the Tailscale client installed and configured to use `TF_VAR_tailscale_auth_key`.

- The `terraform/lab` Terraform scope details:
    - Builds two VPCs (`server` and `client`) and peering between them.
    - Creates an EKS cluster in the `server` VPC and deploys a basic `hello` app plus the Tailscale Operator via Helm.
    - Creates a Linux VM in a private subnet of the `client` VPC that connects to the internet via a NAT gateway and runs the Tailscale client.
    - Provides an optional Tailscale subnet-router instance in the `client` public subnet (toggle with `TF_VAR_tailscale_subnet_router_enable`). When enabled the router will advertise the `client` VPC private subnets to Tailscale using `TF_VAR_tailscale_auth_key`.

- The EKS module used depends on community modules (`terraform-aws-modules/eks/aws`) and the VPC module (`terraform-aws-modules/vpc/aws`). Run `terraform init` to fetch them. These are AWS supported modules that can simplify the code structure, but as modules themselves they are usually trying to do too much, too complex and are shipped with bugs.
- Tailscale auth keys are sensitive: set `TF_VAR_tailscale_auth_key` in environment or a secure `terraform.tfvars` file.
- This lab is intentionally permissive from an IAM perspective and directed at experimentation â€” review IAM and security-group settings before using in production.
- The runner EC2 will require admin permissions on AWS VPC, EC2, EKS, KMS, SSM, IAM and Cloudwatch logs.


### Quick local example (runner):
```bash
cd terraform/runner
terraform init
terraform plan
terraform apply
```

### Quick local example (github-secrets):
```bash
cd terraform/github-secrets
export TF_VAR_github_token="ghp_..."
export TF_VAR_github_owner="my-org"
export TF_VAR_repository="my-repo"
export TF_VAR_aws_access_key_id="..."
export TF_VAR_aws_secret_access_key="..."
export TF_VAR_aws_session_token="..." # optional
terraform init
terraform apply
```

### Quick example to run with GitActions:
Reference: https://docs.github.com/en/actions/how-tos/manage-workflow-runs/manually-run-a-workflow
Please make sure to first upload the GitHub Secrets before proceeding.

Enter the following variables:
```
Terraform target subfolder: lab
Terraform operation: plan/apply/destroy
AWS region: us-west-2
```

## Lessons learned while using the GitHub CoPilot
1. GitHub CoPilot was able to generate the entire setup from less than 5 prompts in less than 15 minutes, but where context was not clear it complicated things such the proper method to install Tailscale, as using a Helm Terraform resource does not seem to be wildly adopted, so it missed important dependencies and premises.
2. The AI assistant also got confused with proper variable escaping and interpolation between Terraform *user_data* scripts and templates in Bash. Bugs!
3. The AI assistant references were somewhat outdated, and that usually adds overhead as the AWS EKS module is quite dynamic and often changes its syntax/arguments quite often. More code review...
4. The overall automagic documentation, comments and diagram /topology as IaC was quite resourceful and helpful.
5. If the AI assistant gets lost on why the EKS is not finishing a deployment or exposing a service due to lack of context, it might suggest troubleshooting via *kubectl*, which will often leads to rabbit holes and complex sub-optimal code changes that force us into a feedback loop of causing another problem while fixing the previous one. Breaking the loop, starting again with fresh code and providing context helped to solve the problems. 


## (Real) Resources
1. https://tailscale.com/kb/1236/kubernetes-operator
2. https://github.com/tailscale/tailscale/blob/main/cmd/k8s-operator/deploy/manifests/operator.yaml
3. https://docs.coreweave.com/docs/products/cks/clusters/coreweave-charts/tailscale-operator
4. https://github.com/rrotaru/eks-over-tailscale/tree/main
5. https://tailscale.com/kb/1196/security-hardening
6. https://tailscale.com/kb/1118/custom-derp-servers
7. https://tailscale.com/kb/1320/performance-best-practices
8. https://aws.amazon.com/blogs/containers/simplify-network-connectivity-using-tailscale-with-amazon-eks-hybrid-nodes/
9. https://serverascode.com/2025/01/31/tailscale-kubernetes-operator.html
10. https://tailscale.com/kb/1210/terraform-provider
11. https://tailscale.com/blog/kubernetes-direct-connections
12. https://github.com/jaxxstorm/tailscale-examples


## What already got a PASS
1. Terraform code for installing secrets, deploying runner and building AWS environment
2. Basic end-to-end connectivity between all peers via Tailscale

## WIP
1. End-to-end testing using subnet router
2. Move Tailscale ACL configs to IaC

## Possible improvements
1. Validate steps to identify NAT types (with/without NAT gateway + relay)
2. Verify whether client-EKS server comms can use the VPC peering path instead of double-NAT + relay
3. Evaluate the impact on latency while using subnet routers instead of relays
4. Move all UI Tailscale settings to IaC
5. Automate connectivity tests
6. Add logging/monitoring with Grafana

